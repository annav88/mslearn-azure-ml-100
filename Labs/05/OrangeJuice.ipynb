{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automated ML Time Series Forecasting for Orange Juice Sales data\n",
        "## Introduction\n",
        "\n",
        "In this notebook we will be using Azure Automated ML in order to predict orange juice sales. The dataset we use is taken from Dominick's Finer Foods, and is available openly.\n",
        "We will first start by checking the installed packages and connecting to the Azure ML workspace which has been created previously.\n",
        "\n",
        "Let us first install the Python SDK v2 for Azure ML : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --pre azure-ai-ml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to upgrade from an existing version, then please use this command below : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --pre --upgrade azure-ai-ml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us check now the version of Azure ML that we installed : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip show azure-ai-ml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a complete list of the installed packages you can use the command **pip list**\n",
        "We will be now importing the required libraries."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also include all the standard ML packages : "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azureml"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to define our subscription ID, resource group name where our Azure ML was created and the Azure ML workspace name : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azureml-core"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Enter details of your AzureML workspace\n",
        "subscription_id = '7567b7de-befe-40fa-b883-40bb316ee50c'\n",
        "resource_group = 'DP100'\n",
        "workspace = 'AzureMLLabTest'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the subscription_id, resource_group and workspace variables defined earlier in order to connect to the Azure ML workspace via SDKv2. We use the MLClient constructor to create an instance of Aure ML client object and use it later on to create our compute instance within that workspace : "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the connection is established, we will now create a compute resource for training. You will notice that we are using the try/except block in order to first check if a compute cluster already exists. If there is no compute cluster, then a new one will be created. You will notice that we will be using the ``` ml_client ``` object that we created in the previous cell, providing the connection to our workspace.\r\n",
        "\r\n",
        "The method ```begin_create_or_update()``` will create an instance of AmlCompute object that we instantiated using the ``` AmlCompute() ``` constructor : \r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\r\n",
        "    compute = AmlCompute(\r\n",
        "        name=cpu_compute_target, size=\"STANDARD_D2_V2\", min_instances=0, max_instances=4\r\n",
        "    )\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# specify aml compute name.\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    ml_client.compute.get(cpu_compute_target)\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "    compute = AmlCompute(\n",
        "        name=cpu_compute_target, size=\"STANDARD_D2_V2\", min_instances=0, max_instances=4\n",
        "    )\n",
        "    ml_client.compute.begin_create_or_update(compute)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "\n",
        "import azureml.core\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data assets\r\n",
        "We will be using open dataset which represents Dominick's store orange juice sales. The dataset contains information about orange juice sales of different brands across different stores. We will be predicting the future sales of orange juice based on the historical data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_column_name = \"WeekStarting\"\r\n",
        "data = pd.read_csv(\"dominicks_OJ_original.csv\", parse_dates=[time_column_name])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featurization_config = FeaturizationConfig()\r\n",
        "featurization_config.blocked_transformers = ['LabelEncoder']\r\n",
        "featurization_config.drop_columns = ['aspiration', 'stroke']\r\n",
        "featurization_config.add_column_purpose('engine-size', 'Numeric')\r\n",
        "featurization_config.add_column_purpose('body-style', 'CategoricalHash')\r\n",
        "#default strategy mean, add transformer param for for 3 columns\r\n",
        "featurization_config.add_transformer_params('Imputer', ['engine-size'], {\"strategy\": \"median\"})\r\n",
        "featurization_config.add_transformer_params('Imputer', ['city-mpg'], {\"strategy\": \"median\"})\r\n",
        "featurization_config.add_transformer_params('Imputer', ['bore'], {\"strategy\": \"most_frequent\"})\r\n",
        "featurization_config.add_transformer_params('HashOneHotEncoder', [], {\"number_of_bits\": 3})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training MLTable defined locally, with local data to be uploaded\r\n",
        "my_training_data_input = Input(\r\n",
        "    type=AssetTypes.MLTABLE, path=\"./\"\r\n",
        ")\r\n",
        "\r\n",
        "# Training MLTable defined locally, with local data to be uploaded\r\n",
        "my_validation_data_input = Input(\r\n",
        "    type=AssetTypes.MLTABLE, path=\"./\"\r\n",
        ")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us first see a sample of the dataset. We can notice that the first column ``` WeekStarting ``` describes the time, there are other columns that describe demographical information about the customers, such as ```Age```. ``` Advert ``` column is a flag defining whether there was a marketing campaign for that data or not."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can notice, the data contains a column *Quantity* that shows how much orange juice was sold per *Store* for a given *Brand*. However, there is a column called *logQuantity* that represents the natural logarithm of the *Quantity* column. This represents a leak into our data and we need to remove that column from our dataset, so that it does not affect our training.\r\n",
        "For this, we will use the ``` drop ``` method, by specifying that we would like to delete a column using the property ```axis = 1 ```. The ``` inplace = True ``` property will let us do the operation inplace, i.e. on the object."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the columns 'logQuantity' as it is a leaky feature.\r\n",
        "data.drop(\"logQuantity\", axis=1, inplace=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the resulting dataframe's sample : "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each combination of the different brand and store there is a different time serie. This means that we need to specify the column ids which determine each of the unique time series for the *Store* and *Brand* combination."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_series_id_column_names = [\"Store\", \"Brand\"]\r\n",
        "nseries = data.groupby(time_series_id_column_names).ngroups\r\n",
        "print(\"Data contains {0} individual time-series.\".format(nseries))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_stores = [2, 5, 8]\r\n",
        "data_subset = data[data.Store.isin(use_stores)]\r\n",
        "nseries = data_subset.groupby(time_series_id_column_names).ngroups\r\n",
        "print(\"Data subset contains {0} individual time-series.\".format(nseries))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are working with time component in our date, the train-test split has to include the time related splitting of the data. We will be splitting the data in time intervals equal to 20."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_test_periods = 20\r\n",
        "\r\n",
        "\r\n",
        "def split_last_n_by_series_id(df, n):\r\n",
        "    \"\"\"Group df by series identifiers and split on last n rows for each group.\"\"\"\r\n",
        "    df_grouped = df.sort_values(time_column_name).groupby(  # Sort by ascending time\r\n",
        "        time_series_id_column_names, group_keys=False\r\n",
        "    )\r\n",
        "    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\r\n",
        "    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\r\n",
        "    return df_head, df_tail\r\n",
        "\r\n",
        "\r\n",
        "train, test = split_last_n_by_series_id(data_subset, n_test_periods)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Featurization ##\r\n",
        "\r\n",
        "We now need to specify the target column and will do some data featurization."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_column_name = \"Quantity\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Below is what we would do for featurization in SDK v1, but this will have to be changed to SDK v2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The below code is using FeaturizationConfig class from SDK v1 which seems to be deprecated\r\n",
        "## or the package is no longer same as in v1\r\n",
        "from azureml.automl.core.featurization import FeaturizationConfig\r\n",
        "\r\n",
        "featurization_config = FeaturizationConfig()\r\n",
        "# Force the CPWVOL5 feature to be numeric type.\r\n",
        "featurization_config.add_column_purpose(\"CPWVOL5\", \"Numeric\")\r\n",
        "# Fill missing values in the target column, Quantity, with zeros.\r\n",
        "featurization_config.add_transformer_params(\r\n",
        "    \"Imputer\", [\"Quantity\"], {\"strategy\": \"constant\", \"fill_value\": 0}\r\n",
        ")\r\n",
        "# Fill missing values in the INCOME column with median value.\r\n",
        "featurization_config.add_transformer_params(\r\n",
        "    \"Imputer\", [\"INCOME\"], {\"strategy\": \"median\"}\r\n",
        ")\r\n",
        "# Fill missing values in the Price column with forward fill (last value carried forward).\r\n",
        "featurization_config.add_transformer_params(\"Imputer\", [\"Price\"], {\"strategy\": \"ffill\"})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Once the featurization is done, my plan is to upload the data as a .csv file \r\n",
        "## into the .\\data folder and then create an ML table"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the algorithm using AutoML SDK\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the compute target is created, we can define the training job. In Azure ML Python SDK v2, we need to define the ```job``` and then submit it.\r\n",
        "\r\n",
        "In order to do that, we need to define a ```command``` where we will specify which Python file has the script for training the algorithm, the input file, hyperparameters, compute target and the environment.\r\n",
        "\r\n",
        "We already created our compute target earlier and will now define the environment and the other components.\r\n",
        "\r\n",
        "We will first use the data that we have featurized before and saved in the .\\data folder. We will create an ML table object which is a series of lazily-evaluated, immutable operations to load data from the data source. Data is not loaded from the source until MLTable is asked to deliver data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\r\n",
        "from azure.ai.ml import automl, Input, MLClient\r\n",
        "\r\n",
        "from azure.ai.ml.constants import AssetTypes\r\n",
        "\r\n",
        "my_training_data_input = Input(\r\n",
        "    type=AssetTypes.MLTABLE, path=\"./data/\"\r\n",
        ")\r\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "forecasting_job = automl.forecasting(\r\n",
        "    compute=cpu_compute_target,\r\n",
        "    # name=\"dpv2-forecasting-job-02\",\r\n",
        "    experiment_name=exp_name,\r\n",
        "    training_data=my_training_data_input,\r\n",
        "    # validation_data = my_validation_data_input,\r\n",
        "    target_column_name=\"Quantity\",\r\n",
        "    primary_metric=\"NormalizedRootMeanSquaredError\",\r\n",
        "    n_cross_validations=3,\r\n",
        "    enable_model_explainability=True,\r\n",
        "    tags={\"my_custom_tag\": \"My custom value\"},\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'automl' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m forecasting_job \u001b[38;5;241m=\u001b[39m \u001b[43mautoml\u001b[49m\u001b[38;5;241m.\u001b[39mforecasting(\n\u001b[1;32m      2\u001b[0m     compute\u001b[38;5;241m=\u001b[39mcpu_compute_target,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# name=\"dpv2-forecasting-job-02\",\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39mexp_name,\n\u001b[1;32m      5\u001b[0m     training_data\u001b[38;5;241m=\u001b[39mmy_training_data_input,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# validation_data = my_validation_data_input,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     target_column_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     primary_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalizedRootMeanSquaredError\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     n_cross_validations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     10\u001b[0m     enable_model_explainability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     tags\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_custom_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy custom value\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     12\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'automl' is not defined"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This is not working, TODO\r\n",
        "### forecast_job = ForecastingJob(primary_metric=primary_metric, forecasting_settings=forecasting_settings, **kwargs)\r\n",
        "\r\n",
        "# Create the AutoML forecasting job with the related factory-function.\r\n",
        "\r\n",
        "forecasting_job = automl.forecasting(\r\n",
        "    compute=cpu_compute_target,\r\n",
        "    # name=\"dpv2-forecasting-job-02\",\r\n",
        "    experiment_name=exp_name,\r\n",
        "    training_data=my_training_data_input,\r\n",
        "    # validation_data = my_validation_data_input,\r\n",
        "    target_column_name=\"Quantity\",\r\n",
        "    primary_metric=\"NormalizedRootMeanSquaredError\",\r\n",
        "    n_cross_validations=3,\r\n",
        "    enable_model_explainability=True,\r\n",
        "    tags={\"my_custom_tag\": \"My custom value\"},\r\n",
        ")\r\n",
        "\r\n",
        "forecasting_job = automl.forecasting(training_data: azure.ai.ml.entities._inputs_outputs.input.Input, \r\n",
        "target_column_name: str, \r\n",
        "primary_metric: str = None, \r\n",
        "enable_model_explainability: bool = None, weight_column_name: str = None, validation_data: azure.ai.ml.entities._inputs_outputs.input.Input = None, validation_data_size: float = None, n_cross_validations: Union[str, int] = None, cv_split_column_names: List[str] = None, test_data: azure.ai.ml.entities._inputs_outputs.input.Input = None, test_data_size: float = None, forecasting_settings: azure.ai.ml.entities._job.automl.tabular.forecasting_settings.ForecastingSettings = None, **kwargs)\r\n",
        "\r\n",
        "# Submit the AutoML job\r\n",
        "returned_job = ml_client.jobs.create_or_update(forecasting_job)  \r\n",
        "returned_job\r\n",
        "\r\n",
        "## The below code is an alternative that I would like to use instead of the above approach, by using command\r\n",
        "\r\n",
        "#job = command(\r\n",
        "#    code=\"./src\",  # local path where the code is stored\r\n",
        "#    command=\"ls ${{inputs.input_data}}\",\r\n",
        "#    inputs=my_job_inputs,\r\n",
        "#    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:9\",\r\n",
        "#    compute=\"cpu-cluster\",\r\n",
        "#)\r\n",
        "\r\n",
        "## submit the command\r\n",
        "#returned_job = ml_client.jobs.create_or_update(job)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## All the below is TODO in SDK v2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving forecasts from the model\r\n",
        "\r\n",
        "We have created a function called run_forecast that submits the test data to the best model determined during the training run and retrieves forecasts. This function uses a helper script forecasting_script which is uploaded and expecuted on the remote compute.\r\n",
        "\r\n",
        "To produce predictions on the test set, we need to know the feature values at all dates in the test set. This requirement is somewhat reasonable for the OJ sales data since the features mainly consist of price, which is usually set in advance, and customer demographics which are approximately constant for each store over the 20 week forecast horizon in the testing data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from run_forecast import run_remote_inference\r\n",
        "\r\n",
        "remote_run_infer = run_remote_inference(\r\n",
        "    test_experiment=test_experiment,\r\n",
        "    compute_target=compute_target,\r\n",
        "    train_run=best_run,\r\n",
        "    test_dataset=test_dataset,\r\n",
        "    target_column_name=target_column_name,\r\n",
        ")\r\n",
        "remote_run_infer.wait_for_completion(show_output=False)\r\n",
        "\r\n",
        "# download the forecast file to the local machine\r\n",
        "remote_run_infer.download_file(\"outputs/predictions.csv\", \"predictions.csv\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load forecast data frame\r\n",
        "fcst_df = pd.read_csv(\"predictions.csv\", parse_dates=[time_column_name])\r\n",
        "fcst_df.head()\r\n",
        "from azureml.automl.core.shared import constants\r\n",
        "from azureml.automl.runtime.shared.score import scoring\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "# use automl scoring module\r\n",
        "scores = scoring.score_regression(\r\n",
        "    y_test=fcst_df[target_column_name],\r\n",
        "    y_pred=fcst_df[\"predicted\"],\r\n",
        "    metrics=list(constants.Metric.SCALAR_REGRESSION_SET),\r\n",
        ")\r\n",
        "\r\n",
        "print(\"[Test data scores]\\n\")\r\n",
        "for key, value in scores.items():\r\n",
        "    print(\"{}:   {:.3f}\".format(key, value))\r\n",
        "\r\n",
        "# Plot outputs\r\n",
        "%matplotlib inline\r\n",
        "test_pred = plt.scatter(fcst_df[target_column_name], fcst_df[\"predicted\"], color=\"b\")\r\n",
        "test_test = plt.scatter(\r\n",
        "    fcst_df[target_column_name], fcst_df[target_column_name], color=\"g\"\r\n",
        ")\r\n",
        "plt.legend(\r\n",
        "    (test_pred, test_test), (\"prediction\", \"truth\"), loc=\"upper left\", fontsize=8\r\n",
        ")\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK V2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b47df59e921b47ad3b0ce713ecc436944faceedaee156a2027efd7b3a1230d4e"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}